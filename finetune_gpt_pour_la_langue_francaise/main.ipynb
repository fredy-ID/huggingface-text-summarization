{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning de gpt2 pour reconnaître le français\n",
    "\n",
    "Ce projet de chatbot IA a pour but d'être intégré dans mon portfolio et être mis à disposition des futurs visiteurs. Le dataset est générés par ChatGPT et ne doit être pris comme étant mon activité mais représente une tentative de finetuner gpt2 pour la langue française.\n",
    "\n",
    "Sources : \n",
    "\n",
    "- [Fine tuning gpt2](https://www.youtube.com/watch?v=elUCn_TFdQc)\n",
    "- [gpt2](https://huggingface.co/gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation de la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatData(Dataset):\n",
    "    def __init__(self, path:str, tokenizer):\n",
    "        self.data = json.load(open(path, \"r\"))\n",
    "\n",
    "        self.X = []\n",
    "        for i in self.data:\n",
    "            for j in i['dialog']:\n",
    "                self.X.append(j['text'])\n",
    "\n",
    "        for idx, i in enumerate(self.X):\n",
    "            try:\n",
    "                self.X[idx] = \"<startofstring> \"+i+\" <bot>: \"+self.X[idx+1]+\" <endofstring>\"\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        self.X = self.X[:3000]\n",
    "        \n",
    "        print(self.X[0])\n",
    "\n",
    "        self.X_encoded = tokenizer(self.X,max_length=40, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        self.input_ids = self.X_encoded['input_ids']\n",
    "        self.attention_mask = self.X_encoded['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.input_ids[idx], self.attention_mask[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \"bos_token\": \"<startofstring>\", \"eos_token\": \"<endofstring>\"})\n",
    "tokenizer.add_tokens([\"<bot>:\"]) \n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(model.generate(**tokenizer('hey i was good at basketball but ', return_tensors='pt'))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> Bonjour ! Je visite votre site portfolio et je suis intÃ©ressÃ© par vos projets. Pouvez-vous me montrer quelques-uns de vos travaux rÃ©cents? <bot>: Bonjour ! Bien sÃ»r, je serais ravi de vous montrer mes projets. Voici l'un de mes projets rÃ©cents : [Nom du Projet 1]. Il s'agit d'un site web interactif pour une entreprise locale. Que pensez-vous de ce projet? <endofstring>\n"
     ]
    }
   ],
   "source": [
    "chatData = ChatData(\"chat_data.json\", tokenizer)\n",
    "chatData =  DataLoader(chatData, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(inp):\n",
    "    inp = \"<startofstring> \"+inp+\" <bot>: \"\n",
    "    inp = tokenizer(inp, return_tensors=\"pt\")\n",
    "    X = inp[\"input_ids\"].to(device)\n",
    "    a = inp[\"attention_mask\"].to(device)\n",
    "    output = model.generate(X, attention_mask=a)\n",
    "    output = tokenizer.decode(output[0])\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(chatData, model, optim):\n",
    "\n",
    "    epochs = 100\n",
    "\n",
    "    for i in tqdm.tqdm(range(epochs)):\n",
    "        for X, a in chatData:\n",
    "            X = X.to(device)\n",
    "            a = a.to(device)\n",
    "            optim.zero_grad()\n",
    "            loss = model(X, attention_mask=a, labels=X).loss\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        torch.save(model.state_dict(), \"model_state.pt\")\n",
    "        print(infer(\"Bonjour !\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50261, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50261, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  8%|▊         | 1/12 [00:27<04:59, 27.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring>Bonjour! <bot>: <pad><pad><pad>ClockWallClock<pad><pad>ClockWall<pad><pad>Clock<pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|█▋        | 2/12 [00:37<02:53, 17.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring>Bonjour! <bot>: -B\n",
      "\n",
      "\n",
      "B-S\n",
      "\n",
      "\n",
      "\n",
      "B\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|██▌       | 3/12 [00:46<02:01, 13.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring>Bonjour! <bot>: ------<startofstring>----<startofstring>-<startofstring>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|███▎      | 4/12 [00:55<01:34, 11.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring>Bonjour! <bot>: <startofstring><startofstring><startofstring><startofstring><startofstring>-<startofstring><startofstring><startofstring><startofstring><startofstring><startofstring><startofstring><startofstring>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|████▏     | 5/12 [01:04<01:15, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring>Bonjour! <bot>:..............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|█████     | 6/12 [01:13<01:00, 10.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring>Bonjour! <bot>: is.............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|█████▊    | 7/12 [01:22<00:48,  9.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring>Bonjour! <bot>: isisisisis.isisisis.isisis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|██████▋   | 8/12 [01:31<00:38,  9.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring>Bonjour! <bot>:..isisisisisisisisisisis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|███████▌  | 9/12 [01:40<00:28,  9.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring>Bonjour! <bot>:..............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|████████▎ | 10/12 [01:49<00:18,  9.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring>Bonjour! <bot>: is....'j..'''''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|█████████▏| 11/12 [01:58<00:09,  9.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring>Bonjour! <bot>: jjjj..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 12/12 [02:07<00:00, 10.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring>Bonjour! <bot>:.'''''''.'.'''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optim = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"training .... \")\n",
    "train(chatData, model, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infer from model : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring>bonjour! <bot>:. de de de de de de de de de de de de de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring>Bonjour! <bot>: -©'''.......'.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fredd\\Documents\\GitDesktop\\Autres\\huggingface-text-summarization\\finetune_gpt\\main.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fredd/Documents/GitDesktop/Autres/huggingface-text-summarization/finetune_gpt/main.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minfer from model : \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fredd/Documents/GitDesktop/Autres/huggingface-text-summarization/finetune_gpt/main.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fredd/Documents/GitDesktop/Autres/huggingface-text-summarization/finetune_gpt/main.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fredd/Documents/GitDesktop/Autres/huggingface-text-summarization/finetune_gpt/main.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   \u001b[39mprint\u001b[39m(infer(inp))\n",
      "File \u001b[1;32mc:\\Users\\fredd\\Documents\\GitDesktop\\Autres\\huggingface-text-summarization\\.env\\Lib\\site-packages\\ipykernel\\kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1201\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[0;32m   1203\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[0;32m   1204\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   1205\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1206\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1207\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fredd\\Documents\\GitDesktop\\Autres\\huggingface-text-summarization\\.env\\Lib\\site-packages\\ipykernel\\kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1245\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1247\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "print(\"infer from model : \")\n",
    "while True:\n",
    "  inp = input()\n",
    "  print(infer(inp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
